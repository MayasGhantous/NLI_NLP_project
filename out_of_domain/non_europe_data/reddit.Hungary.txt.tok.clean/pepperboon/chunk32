  also , separated communities use words differently and this leads to languages splitting up . 
  it would be useful for philosophers to learn programming for this . 
  to learn about pass-by-address , pass-by-value , expressions and evaluations , lazy vs greedy evaluation , etc. 
  you do n't even have to use time here . 
  you could say, 
  * something is `` blight '' if it 's black and it is in my room or white and in your room, 
  * something is `` whack '' if it 's white and it is in my room or black and in your room, 
  if i move my white coffeemug from my room to yours , it suddenly turns from being whack to blight !, 
  what a strange thing !, 
  why is n't being `` whack '' location invariant ? 
  how come that `` white '' is n't location invariant ? 
  but some everyday things do work like blight and whack , for example `` appropriate clothing '' . 
  a fancy dress ceases to be `` appropriate '' the moment you walk out of the ballroom to a muddy forest . 
  are we suprised that the dress suddenly changed ? 
  no , because words always describe mixtures of internal and external properties . 
  or if we want to bring time into this , how about being `` young/old '' ? 
  how is it that we observe someone now for a 1000 times and see that he 's young , but then after 50 years we see he 's old ? 
  seems like `` young '' should be a spooky concept ? 
  some properties are more internal ( size , density ) , some are more external ( location of object , occupation of observer , etc. ) . 
  but no object is perfectly isolated and any property of stuff comes about through interaction with its context/environment . 
  even the size of something depends on the pressure around it . 
  the shape of water depends on the shape of the bowl . 
  sometimes we make our models simpler and imagine that properties of things are either about one thing ( color ) , or the relation about two things ( `` appropriate '' - ness of clothing , depends on both clothes and event ) , but in reality , most concepts are murky and contain connotations about the context . 
  also , philosophically we could argue that nothing `` is '' one way due to something intrinsic , it 's only because the environment allows it to be that way , or better yet , its interplay with the environment gives rise to the property in the first place . 
  it 's a bit like the `` black implies white , good implies bad , i implies other '' sort of vague statements of alan watts . 
  maybe the bike symbol on the road confused them . 
  perhaps they thought it 's only for bikes . 
  completed it . 
  but anyway , theorizing goes only so far . 
  we should n't be too afraid of putting a few avs into operation when they already work well in extensive tests ( like very long driver 's tests ) in increasingly complex environments . 
  then we 'll see what issues come up . 
  we 'll have lots of data , lots of camera pictures and then we can debate about what should have been done instead , and tune the software accordingly . 
  there 's no way around the trial and error , learning by mistakes method . 
  we like to dream up and ponder extreme and weird scenarios with clear-cut options to choose from where all the pressure is on our morals , but reality is rarely like that . 
  the overwhelming majority of it is just mundane , `` hope for the best '' . 
  we like to think we are very moral and live according to principles and when it does n't turn out that way , we have excuses , apologies , forgiveness etc. 
  just make it so that people do n't have to be too afraid and uncertain around these beasts . 
  consistency is key . 
  if everyone knows it kills the people on the road if they step there , then less people will step on the road , we 'll have railings etc. 
  of course this only goes to a certain extent , so we ca n't let the cars go too fast and drive recklessly , just because it 's the pedestrian 's fault if he steps down . 
  we also have to think about the ** incentives ** that each rule gives to the designer/passenger/pedestrian . 
  it 's like an iterated game in game theory . 
  if you make it kill the passenger , then pedestrians will become careless . 
  if you make it kill the pedestrian , then passengers will pressure the designer to make more irresponsible , fast and reckless software . 
  also , let 's not forget that car traffic is already a very very dangerous business . 
  lots of people die , but we since we have someone to point at them and punish them , imprison them , we are okay with it . 
  there is a human to blame , so it 's fine . 
  but if just one person dies from an av , there will be * outrage * , because we ca n't channel our upset feelings into punishing anyone , we 'll fear that the cosmic balance of justice has n't been corrected ... 
  seriously , in the age where the public opinion can be pushed around so much by single well-framed photos ( like the dead syrian boy ) , as opposed to overall statistics ( which are `` just numbers '' ) , we 'll have lots of similar problems in the av topic as well . 
  but it will prevail , i 'm pretty sure . 
   when trained well , and evolved well ( which is a better way of looking at it , since changing the depth of the deep learning network , modifying an objective training function , or broadening the learning sets are not the same as coding some c++ ifchildinroadd ( ) function ) , it will have an emergent behavior which will likely result in less loss of life and serious injury across all scenarios . 
  you are right , but of course not the whole software is black box either . 
  it 's mostly the vision ( detection , recognition ) parts . 
  there will still be explicit rules built in , like what the road signs mean ( it 's not like we just train it and hope it picks up what a stop sign means ) , or how you change lanes ( explicitly programmed checks ) . 
  so it 's a kind of hyprid between `` training and hoping '' and explicit programming . 
  yeah , there are always ways to exploit rules and abuse protected status . 
   2 ) i do n't think it 's hard to imagine a scenario in which the `` kill the occupant '' philosophy would be abused by someone looking to do harm . 
  exactly . 
  whatever seemingly smart rules you have , people can exploit them as long as they are known . 
  i 'm not sure how it 's called , but there 's this paradoxical thing that if an agent acts really rationally and in a principled fashion then it makes it exploitable . 
  for example in the chicken game in game theory , your best bet is to convince the opponent that you 're a lunatic ( with all the catch-22 this implies ) . 
  you ca n't really exploit a piece of rock , or simple machines because they are n't designed to react to your actions in such indirect ways . 
  of course simple machines also `` react '' to what you do , but in a more straightforward , direct way , like if you hit a nail with a hammer , then it `` reacts '' to your `` command '' ( it will become nailed in ) , but by a very direct causal link . 
  however , when a machine is simpler to model in an intentional way , it must not be too naively rational because that 's limiting , it 's powerful but exploitable . 
  so either make it consistent and a bit boneheaded , * or * very cunning and wise , but do n't make it something inbetween because it would be too `` naive '' and exploitable . 
  i agree with you , but this should n't be driven to the extreme either . 
  we should try to avoid damage and risks when we can . 
  just think of how we insulate our wires and do n't just say `` well , learn not to grab on to the 230 v wires in your house , dumbass ! '', 
  we must cut people some slack , we are dumb sometimes . 
  it does n't mean we have to shelter ourselves to the exteme where we get too careless at the expense of others , but we still design industrial machines and everything else in such a way that they are somewhat foolproof . 
  think of the train . 
  we put horns on it , we use barriers and flashing lights at level crossings , etc. 
  these are n't really feasible in the city center . 
  i get that the `` deterrent '' property can work in some cases , too . 
  for example you may explicitly remove every safety feature from some machine to advertise that it needs very very specially trained , very focused operators because it 's dangerous business . 
  so the dilemma is precisely this : where to position avs in this spectrum ? 
  should we give them a more dangerous image so that people are deterred ? 
  but is that extra alertness from pedestrians really worth it ? 
  i mean we have a limited amount of alertness available to us ( `` but it 's trainable and improves with practice '' , sure ) . 
  i mean , if you have a dog and it does something `` wrong '' in a very abstract and complicated sense , you may never be able to teach it not to do that . 
  you may just end up punishing it over and over and over again and it still would n't learn it because it lacks the mental capacity . 
  humanity has some mental limits as well and it 's pointless to scourge them when you 're demanding too much . 
  they will die before they 'd learn . 
   av being controlled by a neigh-omniscient person driving instead of the passenger . 
  this is a good point . 
  people imagine an ai system has automatically transcendent capabilities of looking at the situation from outside the universe , and being able to see every potential outcome and make a perfect decision and all it 's lacking is a weighting function to decide which potential outcome is best . 
   computers are terrible at recognizing patterns ,, 
  they are getting better at it . 
   and the sensors they use do not use visible light . 
  definitely false , they use normal cameras ( besides lidar etc ) . 
  much of computer vision happens on normal consumer-grade cameras and rgb images . 
   that 's like arguing that if a kid climbs a tree and touches a nearby wire the problem is the wire and not the kid . 
  we do put the wires high up , to avoid them being touched . 
  we also try to put dangerous objects far from the reachability of kids . 
  like knives etc. 
  i 'm just trying to say that neither extreme makes sense . 
  but it 's true that we have to accept living with * some * risks because they are n't economical to avoid . 
  it 's just honesty . 
  it 's only inconsistent if you assume that you are n't special . 
  most people do n't think like that .
