  but nevermind , here 's my move . 
  here 's why i believe sentience is nebulous ;, 
  explain to me your version of sentience . 
  what , in your opinion , is real sentience ? 
  no matter what you come up with , your viewpoint will have been disagreed with by any number of philosophers . 
  if it behaves exactly like i do , then it does have a thought process . 
  it would respond to external stimuli , and then process for a bit to find an appropriate response . 
  this is what i do . 
  it 's not . 
  it 's pretty much a simple explanation of what humans do . 
  the human brain is really just an incredibly elaborate set of logic gates , just like any other intelligence , artificial or otherwise . 
  to be a perfect imitation of me , it would also have to experience stimuli that would affect humans as well . 
  its pain nerves would have to activate when it gets injured , it would have to know how to balance itself , or how to detect humidity in the air . 
  it would have to know when it was hungry , or when it was sleepy , or when it was bored or excited . 
  it would not be enough to logically deduce that it must be any of those things , it would have to actually feel them . 
  if it could do all that , then i would have no choice but to call it sapient . 
   we do n't know that , nobody know how it works at this point . 
  nah , we know that it is a set of logic gates . 
  we just do n't know what all of them are . 
  to put it simply , we know that a -  b , but we do n't know why that happens , and what is going on in the meantime to cause that change . 
  granted , neurons are a bit more adaptable than pure logic gates , mainly by their capacity for memory . 
  [ here 's an article about it if you 're interested . 
  ] ( http://systematicpoliticalscience.com/logic.html ), 
   `` to be a perfect imitation of me , it would also have to experience stimuli that would affect humans as well . '', 
   does it have to though ? 
  say if i punch you , you experience this stimuli to make you angry , then your anger causes you to respond by showing anger facial feature then punch me back . 
  you 're not taking into account that humans are not just reactive . 
  by having this machine that seemingly can not feel emotions , then you are skipping out on proactive parts of human decision making . 
  for instance , take a day where i wake up in my bed and have no obligations that day . 
  it is pretty much entirely up to me * when * i decide to leave my bed , as there is no external force prompting me to . 
  perhaps i want to get up to eat breakfast , or to go play video games , or perhaps i would want to sleep some more . 
  this would all depend on my current emotion at the time . 
  now , robo-me has no emotions . 
  if he was a perfect replica of me , he would be spurred to action based on what i would do , right ? 
  the thing is , this is where it would become not so perfect , because while i would take action based on my current emotion , the robo-me would not know what emotion to act upon because he has none , and there are no stimuli telling him what to do . 
  most likely , he would not do anything until something spurs him to action , and that would then contradict how i would act , because i am not an emotionless plank who does n't do anything until something bothers him , no matter what my parents say . 
  to actually be a perfect copy of me , he would have to feel human emotions , so he would know what i would do in that situation . 
  he would not be able to logically deduce or anything , either , because there 's no telling what i would be like . 
  ** tl ; dr ** , robo-me would be ** required ** to feel human emotions to be a perfect replica of me and my behaviour . 
  the link is just what i found first result on a google search . 
  there are other people who deduce that neurons are basically more complex logic gates . 
   is it that hard to write a random number generator to decide what action robo-you take when absence of external stimuli ? 
  you could indeed do that , but that 's an admission that the replica would be imperfect . 
  i am not random in what i decide to do when i leave my bed . 
  the robo-me , lacking human emotions , would have to be to decide what to do , and if he 's random and i 'm not , then he 's not acting as i would . 
  so if we admit that it is an imperfect replica that acts randomly when it is given no impetus , then of course it would n't be a sentient or sapient being . 
  another problem arises , though . 
  it would only act as the true me when it notices that it is being observed , as that would lead to reacting the way i would in the presence of others . 
  as soon as someone inevitably sees it in a moment when it is unprepared , it would act randomly based on behaviours that would be pre-programmed by some people who believe that is the way i would act . 
  it is n't hard to believe that someone would start putting two and two together and realise that something is a little off . 
  especially since all its behaviours have been programmed by other people who believe they have a genuine 1:1 copy of the way i act ( which is incredibly unlikely , but whatever ) , someone noticing a difference is inevitable . 
  the only way it could be completely indistinguishable would be to think as i do . 
  and you did in fact mention that it was perfect in the first comment you made, 
   we program an input-output device that behaves exactly like you, 
  because as you said earlier , the mimic table refers to a random number generator whenever it is called upon to be totally proactive . 
  as these conditions would most likely be met when unobserved , anyone stealth watching robo-me would start witnessing random actions that i would n't do . 
  funny you mention `` all time '' , actually . 
  i had just thought of the ramifications of the mimic table in regards to time . 
  i can accept that the mimic table would be able to accurately mimic present me , but it is literally impossible for it to be able to mimic future me . 
  in 5 years , say , the real me would be thinking differently , and that is pretty much guaranteed considering i have n't even hit my third decade yet . 
  it would be literally ( !! ), 
  impossible to mimic the future me , as while it is guaranteed that i will be thinking differently in the future , it is impossible to know what changes exactly my thinking processes will go through , and how severely it will be affected . 
  this would require being able to read the future in order to make the mimic table accurate ahead of time , unless you have some sort of incredibly accurate human behaviour predictor running alongside the mimic table that would be able to tell how my way of thinking would be affected throughout the years and make changes accordingly . 
  and if you had that , that would pretty much be cognition . 
  to make a robo-me that would be able to accurately and consistently act like me and never once be caught by the innate ability of humans to detect when something 's `` off '' would require a thinking and adaptive intelligence in some capacity . 
   lookup table is a very general thing , it can take multi-year spanning time-sequence as input argument and time-sequence as output argument as well . 
  for example real you made a mistake in the past and learned and adapted to stop making said mistake . 
  the lookup table will simply make the same mistake for the first time and then stop making it to give the illusion of adaptation . 
  i 'll be honest , i have no idea what the hell you 're talking about now . 
  so this robo-me mimics past me ? 
  then it is n't going to fool anyone . 
  if the mistake was already in the table , then the robo-me would already know the outcome of the mistake . 
  why would it make that mistake again ? 
  if it makes the mistake to learn its outcome , then that is n't what an illusion is . 
  that is what actual adaptation is . 
  if i 'm going to be perfectly honest with you , all i 'm seeing is you concede to bigger and bigger contrivances as to how this mimic table can not think for itself . 
  you still have n't answered how it knows to mimic me in the first place . 
  who put those memories there in the first place ? 
  `` if it acts sufficiently like a human , can we conclude it is sentient ? '', 
  perhaps . 
  first , we have to make sure it actually acts like a human , though . 
  mindlessly copying me is not acting like a human . 
  we have machines today that will mindlessly copy you . 
  they are not sentient . 
  to actually act like a human , though ? 
  a clone of me that acts how i would act , yet independently of me ? 
  to literally any degree of practicality , it would be * required * to think . 
  i refuse to believe that you could conjure up a completely static mimic table could be both massively comprehensive and incredibly nuanced enough to perfectly mimic human behaviour . 
  this is like arguing against going outside because you might get struck by a meteor . 
  theoretically possible , but it just is n't going to happen . 
  i 'm not going to let you justify such a garguantuan absurdity with `` theoretically possible '' . 
  it is `` theoretically possible '' that if i toss 100 complete decks of cards off a tower and then pick them up based on which ones are closest to me and all 100 will be completely in order . 
  this is `` theoretically possible '' . 
  so basically , i 'm not impressed with you saying the only way your scenario is valid is because it is n't literally impossible . 
  that is actually ** incredibly ** fucking weak . 
  i 'm going to admit that i 'm not going to continue this argument unless you can come up with something a bit more solid than that . 
  i do n't see how it is expert syndrome . 
  i did not mention any single expert . 
  the thing is , many experts have different interpretations on what `` sentience '' is , and you yourself have your own . 
  it 's arrogant at best , and ignorant at worst , to assume your version of sentience is the correct one .
