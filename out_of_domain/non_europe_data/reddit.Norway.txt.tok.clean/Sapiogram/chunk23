  nvidia does recommend a 500w psu or more , but those recommendations are often very conservative . 
  450w would be just fine unless you want some insanely power hungry cpu . 
  go restore your prayer , man . 
  on the other hand , chess world champion 2016 vishy anand would certainly be something for the history books . 
  obviously perspiration . 
  is it really 1/32768 ? 
   it 's going to happen with intel 's broadwell/skylake architecture . 
  late next year , early 2016 . 
  clarification , intel 's broadwell will only support for [ ddr3 ] ( http://en.wikipedia.org/wiki/broadwell_%28microarchitecture%29 ) , except on broadwell-e . 
   dx-12 is happening around the same timeframe . 
   nothing truly revolutionary is happening on the gpu front after this next wave of cards ( big maxwell and amd 's 20nm cards ) . 
  these cards will be the first to fully support dx-12 , if microsoft is serious about their late 2015 timeframe for the technology . 
  the next new graphics architecture ( after the upcoming one ) probably is n't due until 2017 , so if dx-12 is coming anytime soon the 20nm cards will have to have support for it . 
  `` they '' and `` them '' are both [ borrowings from old norse , ] ( http://en.wikipedia.org/wiki/they#etymology ) compare norwegian bokm\u00e5l `` de '' , `` dem '' and norwegian nynorsk `` dei '' , `` dei '' . 
  his point was that borrowing of such central words is extremely rare , but, 
  he ca n't , there are not herb patches on karamja . 
  in fact , the only way he can train farming afaik is to rake the calquat patch in tai bwo wannai until he gets 72 farming . 
  oh i forgot about that . 
  yeah i guess it 's not completely unreaonable to get 27 farming through weeds and lamps . 
  i 'm not sure how useful it would be though . 
  make curry perhaps ? 
   the average framerate for the last 2 days ( since we released the update ) has been 42 fps . 
  this is 12fps more than the average of a few weeks ago -- that 's like a 25 % improvement . 
  that 's actually a 40 % improvement !, 
  no need to be so modest , garry . 
  yes , that 's the best way to fake such a picture . 
  for big modpacks like direwolf20 , 2 gb is almost unplayable . 
  3gb or more is completely reasonable there . 
  chuck generation is * not * multithreaded . 
  they said they were working on , but it never materialized into anything . 
  chunk reloading is multithreaded , fortunately , but it really only matters when loading the game , or if you are moving ridiculously fast . 
  [ according to the eoc wiki , ] ( http://runescape.wikia.com/wiki/amulet_mould ) they are a drop from certain skeletons . 
  maybe there 's hope ? 
  the sun is too weak in the evenings to have any effect whatsoever . 
  during the day , it only casts a small shadow on nearby valleys , where nothing grows anyway . 
  no lost city , ouch . 
  as cool as `` the ghost cave '' sounds , it will be gone within a few days . 
  it 'll melt and/or be broken apart by the rest of the glacier flowing behind it . 
   the endgame after the correct moves seems hard though . 
  you mean hard for white , right ? 
  [ assuming the line in the top post , and gxh3 is played after ... hxg6 , black gets a rook for its knight , and white is left with two isolated pawns , which allows black to play g5 and get a passed pawn . 
  unless i 'm missing something crucial , that 's a winning position even at club level . 
  ] ( / spoiler ), 
  this is n't as bad as i feared when i read the headline . 
  it only flickers slightly when faced with bad frame stuttering ; specifically , if a frame takes more than 33ms to render . 
  but as i understood it , it 's only very barely noticeable even then . 
  that 's what they are doing , but only after 33ms . 
  that 's already long enough for the pixels to start bleeding very slightly , but forcing a refresh more frequently would kinda remove the whole point of gsync anyway . 
  so as the article said , there is n't really a good solution to this , except making lcd screens bleed less over time . 
  you mean like all non-gsync monitors already work ? 
  once you refresh the monitor , you have to wait another 16.6 ms before you can refresh it again , even if a new frame happens to be ready 1 ms later . 
  which is precisely the problem that gsync tries to fix . 
  normal ( 60hz ) monitors always update every 16.6 ms. 
  this is extremely annoying if your gpu is only capable of delivering a new frame every , let 's say , 18ms : you have nice and smooth 60 fps most of the time , but a few times every second your monitor will pause for an extra frame to wait for the next one to arrive . 
  gsync solves this by always * waiting * for the next frame , updating the screen whenever it arrives . 
  so instead of regular frame drops , you now have nice and smooth 55fps instead . 
  it also gives a nice reduction in input latency , because the monitor is refreshed as soon as the frame is ready , instead of having to wait somewhere between 0-16ms for the screen to actually display it . 
  the problem here is frame stutters . 
  a gsync monitor always optimistically waits for a new frame , because ( in this example ) they almost always arrive every 18ms . 
  if after 33ms ( equivalent to 30 fps ) , the frame still is n't done , they simply re-render the last one like a regular monitor , and you get all the problems mentioned above . 
  so gsync essentially stops working at 30fps . 
  they * could * set the threshold below 33ms , say 25ms , but that would have rendered gsync useless below 40 fps , where many people could need it . 
  although i have n't seen this myself , i imagine this is barely noticeable even with the 33ms threshold , otherwise people would have noticed sooner . 
  gsync has become pretty popular after all . 
  ~ ~ because of qxg7 # ~ ~ misread the comment , see below . 
  oops , sorry , i misread you . 
  rxh7 + nxh7 qxh7 # or ... kg8 qxg7 # , so it 's just postponing the checkmate . 
  that 's because it was deleted , probably automatically by a spam filter . 
  tbh , it 's more like this is a relelvant image for the xkcd . 
  it 's fucking embarrassing that a game being actively developed in 2014 , just is n't playable in anything above 1080p . 
  even the ui elements that do scale , just get stretched instead of using higher resolution textures . 
  i own several 10 year-old games that do it better . 
  man that was an excellent quest . 
  i wan na go do it again . 
  wait , how are you killing the bronze dragons ? 
  just poisoning them and waiting ? 
  we did get gta v , so at least one massively hyped game kinda lived up to itself . 
  on the other hand , the spectactular fiasco that was the sim city reboot ( why , ea , * why ? 
  * ) kinda pulls the year back down . 
  they are also three times cheaper now . 
  life is good as a patient gamer . 
   `` better delayed than broken '' is somewhat a marketing slogan if anything . 
  you delay projects because something broke . 
  some companies \\* * cough ** would still have released it on schedule though . 
  how about compared to a 970 with a good cooler ? 
  or the stock 980 ? 
  i do n't get the downvotes . 
  they 've been out for over a year , people really need to stop calling them `` next-gen '' . 
  is there any particular reason for this ? 
  anyone got a link that works on mobile ? 
  it 's possible to be a great chess player , even in old age . 
  steinitz was world champion until he was 58 , when he `` only '' lost 4-5-10 against lasker ( aged 29 ) , and challenged him again at 60 , which he lost 2-5-10 . 
  sure , it was a different time and age , but the guy was * 58 * . 
   `` replace '' what ? 
  i do n't see the problem . 
  90 % of the time ( conservatively ! ), 
  the type is immediately obvious from context , because you know you are working with strings ( or some other datatype with a replace ( ) method ) . 
  when in doubt , hover over the variables to see their type . 
  it probably still takes less time than mentally parsing those awful names . 
  it would n't be very good at chess if it played exactly the same lines every time .
