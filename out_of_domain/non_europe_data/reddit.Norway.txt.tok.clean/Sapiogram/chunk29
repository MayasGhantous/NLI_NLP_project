  where is this utopia ? 
  very cool article , although if they had waited one more week they could have included two more years of progress ( skylake ) in the charts . 
  hopefully they 'll be added afterwards . 
  i used to do that . 
  it was great until creepers starting spawning when the lights were out . 
  so , for another two days ? 
  honestly , given the recently leaked benchmarks , you probably do n't even need to consider upgrading until next year , when skylake-e , kaby lake and zen offer new hope for noticeable performance improvements . 
  what a terrible site on mobile . 
  i keep getting thrown to another page when trying to scroll . 
  would n't it be more logical for each chunk to store the version it was created in ? 
  different parts of the world might easily be from different versions . 
   why release a crazy 18 core chip that blows everyone out of the water, 
  they do in fact have a server chip with 18 cores , but even if they released it for $ 300 , the 6700k would still be a better buy for most consumers . 
  sure , i 'd * like * one , but it would be very slow in single-threaded workloads . 
  according to intel 's numbers , it 's actually 49 % area shrinkage . 
  so even worse . 
  given amd 's history of cpu launch schedules , 2016 as a whole is optimistic imo . 
  dat sequential write speed though . 
   i would not be upset if mojang took half a year to rewrite the code and not worry about any new things . 
  that 's pretty much what they 've been doing for the last two years now . 
  seriously , dual wielding is pretty much the only notable new feature since 1.7 . 
  doing god 's work here, 
  minecraft is probably running on integrated graphics instead of your gpu , check your gpu control panel . 
  unless your gpu is watercooled or something , there 's absolutely no way it can remain at 40c at load with no fans running . 
  throwing around random ratios like 60-40 is n't really useful , it depends completely on the machine , your settings , and what you 're doing in the game . 
  an average dekstop cpu is only 1.5-2x faster than a laptop gpu ( in single-threaded workloads , like minecraft mostly is ) , while a dedicated desktop gpu can easily be ten times more powerful than their integrated laptop cousins - because of this , laptops tend to be much less cpu limited . 
  additionally , gpu performance in minecraft scales quite well with your render distance , while cpu performance tends to fall apart very quickly as render distance goes up . 
  particularly you 're running around generating lots of new chunks , your cpu will get hit pretty hard . 
  would n't be surprised if they 're already planning this , it 's exactly the kind of thing that becomes viable when you make particles not kill performance . 
  oh no , companies that profit from destroying people 's lives are strictly government regulated . 
  the c ^ 2 is really just a unit conversion though , because humans use arbitrarily defined units . 
  if using [ planck units ] ( https://en.wikipedia.org/wiki/planck_units ) , the equation is just 3 characters : e = m. 
  dude , just stop it ... you 're not outsmarting an engine in an endgame . 
  white then exchanges on d5 , and the b-pawn queens . 
  not sure if you 're serious or making a game of thrones reference . 
   rust is a 64-bit game ( i do n't think 32-bit clients exist anymore ? ), 
  this is a common misconception , but whether the game is running as 32-bit or 64-bit is completely irrelevant in these cases . 
  64-bit programs store numbers in 32 bits all the time , and they will overflow in exactly the same way as in a 32-bit program . 
  the reverse is also true , 32-bit programs can use 64-bit numbers perfectly fine , it 's just a little slower . 
  the programmer has complete control over this , and it works the same whether the program is running as 32-bit or 64-bit . 
  there are exceptions , but they are not important here . 
  they 're 50 million away from 2 ^ 31 and 2 billion away from 2 ^ 32 , i think it 's just a coincidence tbh . 
  how much does john oliver get paid per episode though ? 
  and the rest of his staff ? 
  $ 319 is a drop in the ocean . 
  unfortunately i 'm not hbo . 
   buy out popular browsers, 
  chrome and firefox are open-source , you could n't `` buy '' them in any meaningful way . 
  people would just fork them into a new browser , or modify the new versions to support adblock . 
  also , just the brand names chrome and firefox are worth billions of dollars , i doubt anyone could them out . 
   render any website incompatible with browsers using adblocking, 
  some websites try to do this , but with limited success . 
  in particular , it 's surprisingly hard to detect whether the client even has adblock , and even harder to shut out adblockers effectively . 
   create multiple `` fake '' adblockers that only do a shitty job , i.e. 
  try and drown out the market in shit . 
  not really feasible for any single group or entity to do this while the internet is open for all . 
  `` real '' adblockers would get 10000 upvotes on reddit every other week , and everyone would find them . 
   cooperate with system manufacturers , in order to directly influence operating systems , and what they are compatible with ( presumably updating periodically to account for changing reality ), 
  microsoft tried to use its os monopoly to influence what browsers people use , it did n't end well for them . 
   buy out popular adblockers, 
  possible , but new ones would always surface . 
  adblockers are not terribly complicated , except for the filters , which can be partially crowdsourced . 
   bribe popular adblockers, 
  see above, 
   bribe browser programmers, 
  see my first point, 
  [ version that is n't potato quality . 
  ] ( https://www.youtube.com/watch?v=qv63s6fsx6i ), 
  all i want for christmas is minecraft to get a new engine . 
  i 'm aware that this thread is really old , but for anyone else looking , changing the mouse polling rate to 125 fixed this for me . 
  see [ this . 
  ] ( http://forum.kerbalspaceprogram.com/threads/101374-number-of-issues-running-ksp-on-linux-ubuntu?p=1898556&amp;viewfull=1#post1898556 ), 
  anyone know what move this happened at ? 
  thank you !, 
  that 's a fantastic article for a beginner . 
  what about just having them all on the same day ? 
  got ta sac those queens . 
  you should never resign against someone < 1200 . 
  your opponents make mistakes too ; in this rating range i 've been down a queen several times and still won through my opponent 's blunders . 
  even if it 's just your king left , they might accidentally stalemate you . 
  this goes for < 1500 too tbh , although not to the same extent . 
  you 're contributing to the problem , not the solution . 
   thus humans - who after all are the creators of the tablebases - retain their edge over the machines when it comes to chess . 
  that 's a strange way to say it . 
  it 's not like humans sat around and calculated all the positions in 7-man tablebases , they were generated by computers . 
  sure , the programs * generating * the tables were created by humans , but so were stockfish and the computers running it . 
  you thought too much and too little about this , dude . 
  you 're basically saying that a human using a computer is better than or as good as just a computer . 
  which will always be true , unless the human is really dumb . 
  what about the 15w laptop models ? 
  technically correct i guess, 
  i thought it said ms paint for a second , but i guess that 's too much to hope for . 
   this is not a trivial problem to tackle !, 
  microsoft also has a very non-trivial amount of resources . 
  there are lots of tricks that can be used , especially involving the fact that any given 16 ^ 3 chunk almost never changes . 
  you could imagine many tricks based on this , like caching lots of data about whether there is any way to see through a given cube , etc , and then use that information to unload chunks and also speed up rendering . 
  in a game like minecraft , the sky is limit when it comes to optimizations . 
  that 's a pretty brave statement . 
  java usually does very well in limited benchmarks scenarios , at least cpu-wise , but it really struggles in larger programs . 
  a lot of the it boils down to java being * awful * at memory performance , both in terms of memory locality and just using lots of it , which often goes unnoticed in simpler benchmarks .
